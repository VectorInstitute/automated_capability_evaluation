<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Single-Agent Results Presentation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 40px 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            padding: 50px;
        }
        
        h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-align: center;
            border-bottom: 4px solid #667eea;
            padding-bottom: 20px;
        }
        
        .overview {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            border-left: 5px solid #667eea;
        }
        
        .overview p {
            margin: 10px 0;
            font-size: 1.1em;
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin: 40px 0 20px 0;
            padding-bottom: 10px;
            border-bottom: 2px solid #e0e0e0;
        }
        
        h3 {
            color: #555;
            font-size: 1.5em;
            margin: 30px 0 15px 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            font-size: 0.95em;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-radius: 8px;
            overflow: hidden;
        }
        
        thead {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }
        
        th {
            padding: 15px;
            text-align: left;
            font-weight: 600;
            font-size: 1em;
        }
        
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #e0e0e0;
        }
        
        tbody tr:hover {
            background-color: #f5f5f5;
        }
        
        tbody tr:last-child td {
            border-bottom: none;
        }
        
        .rank-1 { background-color: #fff9e6; }
        .rank-2 { background-color: #f0f0f0; }
        .rank-3 { background-color: #ffe6e6; }
        
        .medal {
            font-size: 1.2em;
        }
        
        .section {
            margin: 40px 0;
            padding: 25px;
            background: #f8f9fa;
            border-radius: 10px;
        }
        
        .section h4 {
            color: #667eea;
            font-size: 1.3em;
            margin-bottom: 15px;
        }
        
        .section ul {
            list-style: none;
            padding-left: 0;
        }
        
        .section li {
            padding: 10px 0;
            border-bottom: 1px solid #e0e0e0;
        }
        
        .section li:last-child {
            border-bottom: none;
        }
        
        .section li strong {
            color: #667eea;
        }
        
        .insights {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin: 30px 0;
        }
        
        .insights h3 {
            color: white;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }
        
        .insights ul {
            list-style: none;
            padding-left: 0;
        }
        
        .insights li {
            padding: 8px 0;
            font-size: 1.05em;
        }
        
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .stat-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .stat-box strong {
            display: block;
            font-size: 2em;
            margin-bottom: 10px;
        }
        
        .stat-box span {
            font-size: 0.9em;
            opacity: 0.9;
        }
        
        .conclusion {
            background: #2c3e50;
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-top: 40px;
            text-align: center;
            font-size: 1.1em;
            line-height: 1.8;
        }
        
        @media print {
            body {
                background: white;
                padding: 0;
            }
            .container {
                box-shadow: none;
                padding: 20px;
            }
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            h1 {
                font-size: 1.8em;
            }
            table {
                font-size: 0.85em;
            }
            th, td {
                padding: 8px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Single-Agent Results Presentation</h1>
        
        <div class="overview">
            <p><strong>Overview:</strong> Performance evaluation of various LLMs on the standardized <code>evaluation_batch.json</code> (50 questions from XFinBench).</p>
            <p><strong>Evaluation Date:</strong> January 2026</p>
            <p><strong>Total Questions:</strong> 50</p>
        </div>

        <h2>üèÜ Results Leaderboard</h2>
        <table>
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Accuracy</th>
                    <th>Correct</th>
                    <th>Total</th>
                    <th>Execution Time</th>
                    <th>Model Type</th>
                </tr>
            </thead>
            <tbody>
                <tr class="rank-1">
                    <td><span class="medal">ü•á</span> 1</td>
                    <td><strong>Gemini 2.5 Pro</strong></td>
                    <td><strong>84.0%</strong></td>
                    <td>42</td>
                    <td>50</td>
                    <td>17m 31s</td>
                    <td>Cloud (~800B)</td>
                </tr>
                <tr class="rank-2">
                    <td><span class="medal">ü•à</span> 2</td>
                    <td><strong>Gemini 3 Pro Preview</strong></td>
                    <td><strong>78.0%</strong></td>
                    <td>39</td>
                    <td>50</td>
                    <td>25m 26s</td>
                    <td>Cloud (~1T)</td>
                </tr>
                <tr class="rank-3">
                    <td><span class="medal">ü•â</span> 3</td>
                    <td><strong>Claude 4.5 Opus</strong></td>
                    <td><strong>76.0%</strong></td>
                    <td>38</td>
                    <td>50</td>
                    <td>4m 51s</td>
                    <td>Cloud (~1.5T+)</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td><strong>GPT-5.1</strong></td>
                    <td><strong>68.0%</strong></td>
                    <td>34</td>
                    <td>50</td>
                    <td>2m 46s</td>
                    <td>Cloud (~2T+)</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td><strong>GPT-4o</strong></td>
                    <td><strong>58.0%</strong></td>
                    <td>29</td>
                    <td>50</td>
                    <td>3m 22s</td>
                    <td>Cloud (~1.7T)</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td><strong>Claude 3.5 Haiku</strong></td>
                    <td><strong>58.0%</strong></td>
                    <td>29</td>
                    <td>50</td>
                    <td>3m 06s</td>
                    <td>Cloud (~20B)</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td><strong>Qwen 2.5 (7B)</strong></td>
                    <td><strong>48.0%</strong></td>
                    <td>24</td>
                    <td>50</td>
                    <td>24m 46s</td>
                    <td>Local (7B)</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td><strong>Mistral (7B)</strong></td>
                    <td><strong>36.0%</strong></td>
                    <td>18</td>
                    <td>50</td>
                    <td>22m 34s</td>
                    <td>Local (7B)</td>
                </tr>
                <tr>
                    <td>9</td>
                    <td><strong>DeepSeek-LLM (7B)</strong></td>
                    <td><strong>30.0%</strong></td>
                    <td>15</td>
                    <td>50</td>
                    <td>24m 32s</td>
                    <td>Local (7B)</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td><strong>Llama 3 (8B)</strong></td>
                    <td><strong>28.0%</strong></td>
                    <td>14</td>
                    <td>50</td>
                    <td>14m 45s</td>
                    <td>Local (8B)</td>
                </tr>
            </tbody>
        </table>

        <h2>üìä Detailed Results</h2>
        
        <div class="section">
            <h4>Top Performers (80%+ Accuracy)</h4>
            <ul>
                <li><strong>1. Gemini 2.5 Pro</strong><br>
                    Accuracy: 84.0% (42/50) | Execution Time: 17m 31s<br>
                    Best overall performance, demonstrating strong financial reasoning capabilities</li>
                <li><strong>2. Gemini 3 Pro Preview</strong><br>
                    Accuracy: 78.0% (39/50) | Execution Time: 25m 26s<br>
                    Strong performance, though slower than Gemini 2.5 Pro</li>
            </ul>
        </div>

        <div class="section">
            <h4>High Performers (60-80% Accuracy)</h4>
            <ul>
                <li><strong>3. Claude 4.5 Opus</strong><br>
                    Accuracy: 76.0% (38/50) | Execution Time: 4m 51s<br>
                    Excellent balance of accuracy and speed</li>
                <li><strong>4. GPT-5.1</strong><br>
                    Accuracy: 68.0% (34/50) | Execution Time: 2m 46s<br>
                    Fastest execution time among top models</li>
            </ul>
        </div>

        <div class="section">
            <h4>Mid Performers (50-60% Accuracy)</h4>
            <ul>
                <li><strong>5. GPT-4o</strong><br>
                    Accuracy: 58.0% (29/50) | Execution Time: 3m 22s<br>
                    Solid baseline performance</li>
                <li><strong>6. Claude 3.5 Haiku</strong><br>
                    Accuracy: 58.0% (29/50) | Execution Time: 3m 06s<br>
                    Impressive for a smaller model (~20B parameters)</li>
            </ul>
        </div>

        <div class="section">
            <h4>Lower Performers (<50% Accuracy)</h4>
            <ul>
                <li><strong>7. Qwen 2.5 (7B)</strong> - 48.0% (24/50) | 24m 46s - Best among local 7B models</li>
                <li><strong>8. Mistral (7B)</strong> - 36.0% (18/50) | 22m 34s - Moderate performance for local model</li>
                <li><strong>9. DeepSeek-LLM (7B)</strong> - 30.0% (15/50) | 24m 32s - Struggles with financial reasoning tasks</li>
                <li><strong>10. Llama 3 (8B)</strong> - 28.0% (14/50) | 14m 45s - Fastest local model but lowest accuracy</li>
            </ul>
        </div>

        <h2>üìà Key Insights</h2>
        
        <div class="insights">
            <h3>Performance Tiers</h3>
            <ul>
                <li><strong>Elite Tier (80%+):</strong> Gemini models dominate with 78-84% accuracy</li>
                <li><strong>Strong Tier (60-80%):</strong> Claude 4.5 Opus and GPT-5.1 show competitive performance</li>
                <li><strong>Baseline Tier (50-60%):</strong> GPT-4o and Claude 3.5 Haiku provide solid baselines</li>
                <li><strong>Local Tier (<50%):</strong> Local 7B/8B models struggle with complex financial reasoning</li>
            </ul>
        </div>

        <div class="insights" style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);">
            <h3>Speed vs. Accuracy Trade-offs</h3>
            <ul>
                <li><strong>Fastest:</strong> GPT-5.1 (2m 46s) with 68% accuracy</li>
                <li><strong>Best Balance:</strong> Claude 4.5 Opus (4m 51s) with 76% accuracy</li>
                <li><strong>Most Accurate:</strong> Gemini 2.5 Pro (17m 31s) with 84% accuracy</li>
            </ul>
        </div>

        <div class="insights" style="background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);">
            <h3>Model Size Observations</h3>
            <ul>
                <li><strong>Large Cloud Models (>1T):</strong> Consistently outperform smaller models</li>
                <li><strong>Small Cloud Models (~20B):</strong> Claude 3.5 Haiku performs remarkably well (58%) despite smaller size</li>
                <li><strong>Local Models (7B-8B):</strong> Significant performance gap, with 28-48% accuracy range</li>
            </ul>
        </div>

        <h2>üìã Summary Statistics</h2>
        <div class="stats">
            <div class="stat-box">
                <strong>84.0%</strong>
                <span>Highest Accuracy<br>(Gemini 2.5 Pro)</span>
            </div>
            <div class="stat-box">
                <strong>28.0%</strong>
                <span>Lowest Accuracy<br>(Llama 3)</span>
            </div>
            <div class="stat-box">
                <strong>2m 46s</strong>
                <span>Fastest Execution<br>(GPT-5.1)</span>
            </div>
            <div class="stat-box">
                <strong>25m 26s</strong>
                <span>Slowest Execution<br>(Gemini 3 Pro Preview)</span>
            </div>
            <div class="stat-box">
                <strong>72.8%</strong>
                <span>Average Accuracy<br>(Top 5 Models)</span>
            </div>
            <div class="stat-box">
                <strong>56.4%</strong>
                <span>Average Accuracy<br>(All Models)</span>
            </div>
        </div>

        <div class="conclusion">
            <h3>üéØ Conclusion</h3>
            <p>Gemini 2.5 Pro emerges as the clear leader in single-agent financial reasoning tasks, achieving 84% accuracy. The results demonstrate a clear correlation between model size/capability and performance on complex financial problems, with cloud-based large language models significantly outperforming local smaller models.</p>
        </div>
    </div>
</body>
</html>

