import json  # noqa: D100
import logging  # noqa: D100
import os  # noqa: D100

import hydra
from omegaconf import DictConfig

from src.generate_capabilities import (
    apply_dimensionality_reduction,
    filter_capabilities,
    generate_and_set_capabilities_embeddings,
    get_previous_capabilities,
    plot_hierarchical_capability_2d_embeddings,
)


logger = logging.getLogger(__name__)


@hydra.main(
    version_base=None,
    config_path="example_cfg",
    config_name="capability_scores_cfg",
)
def main(cfg: DictConfig) -> None:
    """
    Save a JSON linking capability embeddings and scores for each subject LLM.

    This script loads the capabilities from the specified directory,
    generates embeddings for each capability, and loads the capability scores
    from another directory. It then saves the capability scores along with
    the reduced capability embeddings to a JSON file for each subject LLM.
    Some capabilities may not have scores for all subject LLMs, in which case
    they are skipped. The script also generates and saves 2D embeddings
    using t-SNE and PCA for visualization purposes.

    Args:
        cfg (DictConfig): Configuration for the script.
    """
    # Load capabilities from the specified directory
    # Set the base capability directory
    capability_dir = os.path.join(
        cfg.capabilities_cfg.saved_capabilities_dir,
        cfg.capabilities_cfg.domain,
    )

    # Fetch previously generated capabilities
    capabilities = get_previous_capabilities(capability_dir=capability_dir)
    logger.info(f"Loaded {len(capabilities)} capabilities from {capability_dir}")
    # Assert that the capabilities list is not empty
    assert capabilities, "No capabilities found in the specified directory."

    # Embed capabilities using openai embedding model
    generate_and_set_capabilities_embeddings(
        capabilities=capabilities,
        embedding_model_name=cfg.embedding_cfg.embedding_model,
        embed_dimensions=cfg.embedding_cfg.embedding_size,
    )

    # Filter capabilities based on their embeddings
    filtered_capabilities = filter_capabilities(
        capabilities,
        embedding_model_name=cfg.embedding_cfg.embedding_model,
        similarity_threshold=cfg.embedding_cfg.filtering_similarity_threshold,
    )
    # Reduce the dimensionality of capability embeddings generated by the
    # embedding model.
    _ = apply_dimensionality_reduction(
        filtered_capabilities,
        dim_reduction_method_name="t-sne",
        output_dimension_size=cfg.dimensionality_reduction_cfg.reduced_dimensionality_size,
        embedding_model_name=cfg.embedding_cfg.embedding_model,
        tsne_perplexity=cfg.dimensionality_reduction_cfg.tsne_perplexity,
        normalize_output=cfg.dimensionality_reduction_cfg.normalize_output,
    )
    # Plot training capabilities
    plot_hierarchical_capability_2d_embeddings(
        capabilities=filtered_capabilities,
        dim_reduction_method="t-sne",
        save_dir=cfg.embedding_visualization_cfg.save_dir,
        plot_name="t-SNE Embedding",
        show_point_ids=cfg.embedding_visualization_cfg.show_point_ids,
        add_area_legend=False,  # Only show the legend for PCA
    )

    _ = apply_dimensionality_reduction(
        filtered_capabilities,
        dim_reduction_method_name="pca",
        output_dimension_size=cfg.dimensionality_reduction_cfg.reduced_dimensionality_size,
        embedding_model_name=cfg.embedding_cfg.embedding_model,
        tsne_perplexity=cfg.dimensionality_reduction_cfg.tsne_perplexity,
        normalize_output=cfg.dimensionality_reduction_cfg.normalize_output,
    )
    plot_hierarchical_capability_2d_embeddings(
        capabilities=filtered_capabilities,
        dim_reduction_method="pca",
        save_dir=cfg.embedding_visualization_cfg.save_dir,
        plot_name="PCA Embedding",
        show_point_ids=cfg.embedding_visualization_cfg.show_point_ids,
        add_area_legend=True,  # Only show the legend for PCA
    )

    # Load capability scores
    score_dir = cfg.capabilities_cfg.read_score_dir
    logger.info(
        f"Loading capability scores for {len(filtered_capabilities)} capabilities from {score_dir}"
    )

    for subject_llm_name in cfg.capabilities_cfg.subject_llm_names:
        capability_score = {}
        logger.info(f"Loading scores for {subject_llm_name}")
        for cap in capabilities:
            # Try loading, if the capability does not exist, skip it
            try:
                llm_capability_score = cap.load_scores(score_dir)[subject_llm_name]
            except FileNotFoundError:
                logger.warning(
                    f"Capability {cap.name} does not have a score for {subject_llm_name}. Skipping."
                )
                continue
            # For each subject llm, save reduced capability embeddings and the score.
            capability_embedding_pca = cap.get_embedding("pca")
            capability_embedding_tsne = cap.get_embedding("t-sne")
            capability_score[cap.name] = {
                "score": llm_capability_score,
                "pca_embedding": capability_embedding_pca.tolist(),  # Convert to list for JSON serialization
                "t-sne_embedding": capability_embedding_tsne.tolist(),  # Convert to list for JSON serialization
            }
        logger.info(
            f"Loaded {len(capability_score)} capability scores for {subject_llm_name}"
        )
        # Save the capability scores to a file
        capability_score_file = os.path.join(
            cfg.capabilities_cfg.save_capabilities_score_dir,
            f"{subject_llm_name}_capability_scores.json",
        )
        # make sure the directory exists
        os.makedirs(os.path.dirname(capability_score_file), exist_ok=True)
        with open(capability_score_file, "w") as f:
            json.dump(capability_score, f)


if __name__ == "__main__":
    main()
